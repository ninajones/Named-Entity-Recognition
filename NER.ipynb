{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the data that you will be making predictions on. Let’s use this dataset containing titles, text, etc of Medium articles: https://www.kaggle.com/hsankesara/medium-articles#articles.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"articles.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean the dataset that the predictions will be made on. The column ‘text’ contains the content of each Medium article, so we will only need to clean that column. First examine the column to see which cleaning steps need to be taken. I applied the following changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(r'\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(r'\\\\n', ' ')\n",
    "df['text'] = df['text'].str.replace(r'\\\\t', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(r'[^\\x00-\\x7F]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(\"<!--.*-->\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the pickled tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tags = None\n",
    "with open('tags.pickle', 'rb') as f:\n",
    "     tags = pickle.load(f)\n",
    "\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Read in some more of the code used in the training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a tensorflow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download bi-directional LSTM model pretrained with ELMo word embeddings to learn both word (e.g., syntax and semantics) and linguistic context of a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that vectorizes a sequence of strings with the ELMo embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set the max sentence length. It will be different than in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set batch size. It will be different than in the training script. It must be divisable by 32 because sequence_len is dtype=int32 and so needs to be made shape=(32,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a residual LSTM network with an ELMo embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_text, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load in the trained model that you previously saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./testmodel_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Function which adds word padding to prediction dataset. Different from training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from typing import List\n",
    "\n",
    "def add_word_padding(words: List[str], words_per_sent = 200, pad_str = '__PAD__') -> List[str]:\n",
    "    num_pad_words = words_per_sent - len(words)\n",
    "    pad_words = ['__PAD__'] * num_pad_words\n",
    "    return words + pad_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Function which adds sentence padding to prediction dataset. \n",
    "\n",
    "### Each article is 512 sentences length. Both batch size and sent_per_article must be divisable by 32 because sequence_len is dtype=int32 and so needs to be made shape=(32,).\n",
    "\n",
    "### If you know that the max sentence length of each article is quite a bit below 512 words, you should definitely lower sent_per_article. Of course, the lower the sentence length, the faster the prediction will run. However, make sure that you keep sent_per_article divisable by 32, so if the longest sentence length is 300 words, change sent_per_article to the next number above that which is divisible by 32 (so, in that case, 320).\n",
    "\n",
    "### If you know that the max word length of each sentence of each article is say 100, lower words_per_sent to 100. There is no need for words_per_sent to be divisable by a particular number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pad_sentences(sentences: List[List[str]], words_per_sent = 200, sent_per_article = 512, pad_str = '__PAD__') -> List[List[str]]:\n",
    "    pad_sent = ['__PAD__'] * words_per_sent\n",
    "    num_pad_sentences = sent_per_article - len(sentences)\n",
    "    pad_sentences =  [pad_sent] * num_pad_sentences\n",
    "    return pad_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Preprocessing function which splits an article into a list of sentences, then splits each sentence into a list of words, and then applies word and sentence padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_input(article: str) -> List:\n",
    "    # split article into list of sentences\n",
    "    sentences: List[str] = sent_tokenize(article)\n",
    "    \n",
    "    # split sentences into list of words\n",
    "    sentences_split_into_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "   \n",
    "    # add padding to words\n",
    "    pad_words = [add_word_padding(sentence) for sentence in sentences_split_into_words]\n",
    "\n",
    "    # add padding to sentences\n",
    "    return pad_words + gen_pad_sentences(pad_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Predicting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_article(article):\n",
    "    #create list of lists\n",
    "    preprocessed_article = np.array(create_nn_input(article))\n",
    "    predictions = model.predict(preprocessed_article)\n",
    "    # picks best tag for each word\n",
    "    predicted_label_indices = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # gets tags of each element of a\n",
    "    convert_tags = np.vectorize(lambda x: tags[x])\n",
    "    #apply convert_tags on predicted_label_indices\n",
    "    predicted_tags = convert_tags(predicted_label_indices)\n",
    "\n",
    "    #todo: merge b and i pairs\n",
    "    #2d array for labels create pairwise tuples with 2d array of words\n",
    "    flat_words = preprocessed_article.flatten()\n",
    "    flat_tags = predicted_tags.flatten()\n",
    "    \n",
    "    combine_tags_words = list(zip(flat_tags, flat_words))    \n",
    "  \n",
    "    #function, for each tuple (lbl, word) in result, keep tuples where label (t[0]) not O\n",
    "    return list(filter(lambda t: t[0] != 'O', combine_tags_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Apply predicting function on each text row in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = df.apply(lambda row: predict_article(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you have made the predictions and they are in lists of tuples. Each tuple contains a word and tag pair. Each list of tuples corresponds to a different Medium article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Next we will format the predictions and add them to the predicting dataset under a new column called “Entities”. Formatting them will allow us to more clearly see the entities extracted from each Medium article.\n",
    "\n",
    "### First convert prediction into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a bunch of functions that format the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tags(x):\n",
    "    # Change list of tuples into list\n",
    "    nested_list = [item for t in x for item in t]\n",
    "\n",
    "    # Add string 'split' before each tag containing B in 0th index of tag\n",
    "    res = []\n",
    "    for entry in nested_list:\n",
    "        if entry[0:2] == 'B-':\n",
    "            res.append('split')\n",
    "        res.append(entry)\n",
    "    nested_list[:] = res\n",
    "    res\n",
    "    \n",
    "    # Split by string 'split'\n",
    "    def split_condition(x):\n",
    "        return x in {'split'}\n",
    "\n",
    "    grouper = groupby(res, key=split_condition)\n",
    "\n",
    "    # Convert to dictionary via enumerate\n",
    "    conv_to_dict = dict(enumerate((list(j) for i, j in grouper if not i), 1))\n",
    "        \n",
    "    # Convert dictionary of lists into list of lists\n",
    "    dictionary_to_list = [[k]+v for k,v in conv_to_dict.items()]\n",
    "\n",
    "    # Remove first element in each list\n",
    "    for x in dictionary_to_list:\n",
    "        del x[0]\n",
    "    \n",
    "    return dictionary_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply merge_tags function to each list in list\n",
    "def apply_merge_tags(x):\n",
    "    return [replace_B(l) for l in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tag type for B if group contains two I's, then drop the I's. \n",
    "# Otherwise, just drop the I's.\n",
    "def replace_B(x):\n",
    "    result = x\n",
    "\n",
    "    #get all I-'s\n",
    "    I_tags = list(filter(lambda x: x[:2] == 'I-', x))\n",
    "    if len(I_tags) == 2:\n",
    "        second_to_last_I_tag = I_tags[-2]\n",
    "        new_B_tag = 'B' + second_to_last_I_tag[1:]\n",
    "        bb_no_Is = list(filter(lambda x: x[:2] != 'I-', x))\n",
    "        result = bb_no_Is.copy()\n",
    "        result[0] = new_B_tag\n",
    "    else:\n",
    "        bb_no_Is = list(filter(lambda x: x[:2] != 'I-', x))\n",
    "        result = bb_no_Is.copy()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change B- tags with more readable label names\n",
    "def change_label(label):\n",
    "    if label == 'B-geo':\n",
    "        return 'Location:'\n",
    "    elif label == 'B-gpe':\n",
    "        return 'Geopolitical Entity:'\n",
    "    elif label == 'B-org':\n",
    "        return 'Company:'\n",
    "    elif label == 'B-per':\n",
    "        return 'Person:'\n",
    "    elif label == 'B-tim':\n",
    "        return 'Time Period:'\n",
    "    else: return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply change_label function to first element of each list \n",
    "# Then change that element to show the result of applied function\n",
    "def apply_change_label(y):\n",
    "    for x in y:\n",
    "        x[0] = change_label(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all but first element into string\n",
    "def join_string(y):\n",
    "    return [[x[0]] + [' '.join(x[1:])] for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge lists with same first element and create dictionary\n",
    "def merge_lists(x):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    for i, j in x:\n",
    "        d[i].append(j)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the functions to each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.apply(lambda row: merge_tags(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction to df\n",
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.apply(lambda row: apply_merge_tags(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction to df\n",
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.apply(lambda row: replace_B(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction to df\n",
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.apply(lambda row: change_label(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction to df\n",
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.apply(lambda row: apply_change_label(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.apply(lambda row: join_string(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction to df\n",
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply final function to prediction and apply it to new column in df called Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Entities\"] = prediction.apply(lambda row: merge_lists(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you have a dataset of Medium articles with a new Entities column containing the formatted entities for each article. These entities can be viewed on the front end by converting the entities (nested list format) into JSON objects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
